{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnsRt65dP5S2W0Hmvn9R4M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import time, random\n","from urllib.parse import urlencode\n","import requests\n","import xml.etree.ElementTree as ET\n","from typing import List, Dict, Any  # <-- quan trọng\n","\n","HEADERS = {\"User-Agent\": \"AcademicCrawler/1.0 (+mailto:youremail@example.com)\"}\n","API_ENDPOINT = \"https://export.arxiv.org/api/query\"\n","\n","def get_html(url: str, timeout: int = 20) -> str:\n","    try:\n","        r = requests.get(url, headers=HEADERS, timeout=timeout)\n","        return r.text if r.status_code == 200 else \"\"\n","    except Exception:\n","        return \"\"\n","\n","def arxiv_api_search(\n","    query: str,\n","    max_results: int = 200,\n","    page_size: int = 100,\n","    sortBy: str = \"submittedDate\",\n","    sortOrder: str = \"descending\",\n","    delay_sec: float = 1.0,\n",") -> List[Dict[str, Any]]:\n","    entries: List[Dict[str, Any]] = []\n","    retrieved = 0\n","    while retrieved < max_results:\n","        size = min(page_size, max_results - retrieved)\n","        params = {\n","            \"search_query\": query,\n","            \"start\": retrieved,\n","            \"max_results\": size,\n","            \"sortBy\": sortBy,\n","            \"sortOrder\": sortOrder,\n","        }\n","        xml_text = get_html(f\"{API_ENDPOINT}?{urlencode(params)}\")\n","        if not xml_text:\n","            break\n","\n","        ns = {\"atom\": \"http://www.w3.org/2005/Atom\"}\n","        root = ET.fromstring(xml_text)\n","        feed_entries = root.findall(\"atom:entry\", ns)\n","        if not feed_entries:\n","            break\n","\n","        for e in feed_entries:\n","            arxiv_id = e.find(\"atom:id\", ns).text.rsplit(\"/\", 1)[-1]\n","            title    = (e.find(\"atom:title\", ns).text or \"\").strip()\n","            summary  = (e.find(\"atom:summary\", ns).text or \"\").strip()\n","            authors  = [a.find(\"atom:name\", ns).text for a in e.findall(\"atom:author\", ns)]\n","            pdf_link = \"\"\n","            detail   = \"\"\n","            for l in e.findall(\"atom:link\", ns):\n","                if l.attrib.get(\"title\") == \"pdf\" or l.attrib.get(\"type\") == \"application/pdf\":\n","                    pdf_link = l.attrib.get(\"href\", \"\")\n","                if l.attrib.get(\"rel\") == \"alternate\":\n","                    detail = l.attrib.get(\"href\", \"\")\n","\n","            cats = [c.attrib.get(\"term\") for c in e.findall(\"{http://www.w3.org/2005/Atom}category\")]\n","            updated = (e.find(\"atom:updated\", ns).text or \"\")\n","            published = (e.find(\"atom:published\", ns).text or \"\")\n","\n","            entries.append({\n","                \"arXiv ID\": arxiv_id,\n","                \"Title\": title,\n","                \"Authors\": \", \".join(authors),\n","                \"Subjects\": \", \".join(cats),\n","                \"Subject_Tags\": \", \".join(cats),\n","                \"Abstract\": summary,\n","                \"Submitted\": f\"Published: {published} | Updated: {updated}\",\n","                \"Detail Link\": detail,\n","                \"PDF Link\": pdf_link,\n","            })\n","        retrieved += len(feed_entries)\n","        time.sleep(delay_sec)\n","    return list({e[\"arXiv ID\"]: e for e in entries}.values())\n"],"metadata":{"id":"fd_ta2EXtIS4","executionInfo":{"status":"ok","timestamp":1759225555926,"user_tz":-420,"elapsed":48,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Nhập query ở đây\n","query = 'ti:\"vision transformer\" AND cat:cs.CV'\n","max_results = 200\n","page_size   = 100\n","delay_sec   = 1.2   # nên >= 1.0 để tôn trọng rate limit arXiv\n","\n","api_results = arxiv_api_search(\n","    query=query,\n","    max_results=max_results,\n","    page_size=page_size,\n","    delay_sec=delay_sec,\n",")\n","\n","print(f\"API collected: {len(api_results)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEtCXMvHHV3a","executionInfo":{"status":"ok","timestamp":1759227258143,"user_tz":-420,"elapsed":3520,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"e2c3885c-4ecb-4927-adda-0d912bbed1ce"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["API collected: 200\n"]}]},{"cell_type":"code","source":["import json, pandas as pd\n","\n","def save_results(items, csv_path=\"arxiv_api.csv\", jsonl_path=\"arxiv_api.jsonl\"):\n","    if not items:\n","        print(\"No items to save.\"); return\n","    # CSV\n","    df = pd.DataFrame(items)\n","    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n","    # JSONL\n","    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n","        for row in items:\n","            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n","    print(f\"Saved {len(items)} records -> {csv_path} & {jsonl_path}\")\n","\n","# Xem 3 dòng đầu\n","pd.DataFrame(api_results).head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"1rFcbYxsHdAB","executionInfo":{"status":"ok","timestamp":1759227436102,"user_tz":-420,"elapsed":52,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"bb8a1b0e-9988-4a06-8758-e994e4d5ca2b"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       arXiv ID                                              Title  \\\n","0  2509.23859v1  FairViT-GAN: A Hybrid Vision Transformer with ...   \n","1  2509.23751v1  PVTAdpNet: Polyp Segmentation using Pyramid vi...   \n","2  2509.23235v1  Patch Rebirth: Toward Fast and Transferable Mo...   \n","3  2509.21084v1  Vision Transformers: the threat of realistic a...   \n","4  2509.20986v2  SiNGER: A Clearer Voice Distills Vision Transf...   \n","\n","                                             Authors      Subjects  \\\n","0                             Djamel Eddine Boukhari         cs.CV   \n","1  Arshia Yousefi Nezhad, Helia Aghaei, Hedieh Sa...  cs.CV, cs.AI   \n","2                        Seongsoo Heo, Dong-Wan Choi  cs.CV, cs.AI   \n","3  Kasper Cools, Clara Maathuis, Alexander M. van...  cs.CV, cs.AI   \n","4  Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Ja...  cs.CV, cs.AI   \n","\n","   Subject_Tags                                           Abstract  \\\n","0         cs.CV  Facial Beauty Prediction (FBP) has made signif...   \n","1  cs.CV, cs.AI  Colorectal cancer ranks among the most common ...   \n","2  cs.CV, cs.AI  Model inversion is a widely adopted technique ...   \n","3  cs.CV, cs.AI  The increasing reliance on machine learning sy...   \n","4  cs.CV, cs.AI  Vision Transformers are widely adopted as the ...   \n","\n","                                           Submitted  \\\n","0  Published: 2025-09-28T12:55:31Z | Updated: 202...   \n","1  Published: 2025-09-28T08:55:50Z | Updated: 202...   \n","2  Published: 2025-09-27T10:35:44Z | Updated: 202...   \n","3  Published: 2025-09-25T12:36:25Z | Updated: 202...   \n","4  Published: 2025-09-25T10:29:47Z | Updated: 202...   \n","\n","                         Detail Link                           PDF Link  \n","0  http://arxiv.org/abs/2509.23859v1  http://arxiv.org/pdf/2509.23859v1  \n","1  http://arxiv.org/abs/2509.23751v1  http://arxiv.org/pdf/2509.23751v1  \n","2  http://arxiv.org/abs/2509.23235v1  http://arxiv.org/pdf/2509.23235v1  \n","3  http://arxiv.org/abs/2509.21084v1  http://arxiv.org/pdf/2509.21084v1  \n","4  http://arxiv.org/abs/2509.20986v2  http://arxiv.org/pdf/2509.20986v2  "],"text/html":["\n","  <div id=\"df-645f3132-0ebe-4156-8b0c-afa92280b533\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>arXiv ID</th>\n","      <th>Title</th>\n","      <th>Authors</th>\n","      <th>Subjects</th>\n","      <th>Subject_Tags</th>\n","      <th>Abstract</th>\n","      <th>Submitted</th>\n","      <th>Detail Link</th>\n","      <th>PDF Link</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2509.23859v1</td>\n","      <td>FairViT-GAN: A Hybrid Vision Transformer with ...</td>\n","      <td>Djamel Eddine Boukhari</td>\n","      <td>cs.CV</td>\n","      <td>cs.CV</td>\n","      <td>Facial Beauty Prediction (FBP) has made signif...</td>\n","      <td>Published: 2025-09-28T12:55:31Z | Updated: 202...</td>\n","      <td>http://arxiv.org/abs/2509.23859v1</td>\n","      <td>http://arxiv.org/pdf/2509.23859v1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2509.23751v1</td>\n","      <td>PVTAdpNet: Polyp Segmentation using Pyramid vi...</td>\n","      <td>Arshia Yousefi Nezhad, Helia Aghaei, Hedieh Sa...</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>Colorectal cancer ranks among the most common ...</td>\n","      <td>Published: 2025-09-28T08:55:50Z | Updated: 202...</td>\n","      <td>http://arxiv.org/abs/2509.23751v1</td>\n","      <td>http://arxiv.org/pdf/2509.23751v1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2509.23235v1</td>\n","      <td>Patch Rebirth: Toward Fast and Transferable Mo...</td>\n","      <td>Seongsoo Heo, Dong-Wan Choi</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>Model inversion is a widely adopted technique ...</td>\n","      <td>Published: 2025-09-27T10:35:44Z | Updated: 202...</td>\n","      <td>http://arxiv.org/abs/2509.23235v1</td>\n","      <td>http://arxiv.org/pdf/2509.23235v1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2509.21084v1</td>\n","      <td>Vision Transformers: the threat of realistic a...</td>\n","      <td>Kasper Cools, Clara Maathuis, Alexander M. van...</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>The increasing reliance on machine learning sy...</td>\n","      <td>Published: 2025-09-25T12:36:25Z | Updated: 202...</td>\n","      <td>http://arxiv.org/abs/2509.21084v1</td>\n","      <td>http://arxiv.org/pdf/2509.21084v1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2509.20986v2</td>\n","      <td>SiNGER: A Clearer Voice Distills Vision Transf...</td>\n","      <td>Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Ja...</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>cs.CV, cs.AI</td>\n","      <td>Vision Transformers are widely adopted as the ...</td>\n","      <td>Published: 2025-09-25T10:29:47Z | Updated: 202...</td>\n","      <td>http://arxiv.org/abs/2509.20986v2</td>\n","      <td>http://arxiv.org/pdf/2509.20986v2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-645f3132-0ebe-4156-8b0c-afa92280b533')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-645f3132-0ebe-4156-8b0c-afa92280b533 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-645f3132-0ebe-4156-8b0c-afa92280b533');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-4c648c79-6175-46bd-81d2-dba1e9a1ea41\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c648c79-6175-46bd-81d2-dba1e9a1ea41')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-4c648c79-6175-46bd-81d2-dba1e9a1ea41 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"arXiv ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2509.23751v1\",\n          \"2509.20986v2\",\n          \"2509.23235v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a   novel Adapter block\",\n          \"SiNGER: A Clearer Voice Distills Vision Transformers Further\",\n          \"Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision   Transformers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Arshia Yousefi Nezhad, Helia Aghaei, Hedieh Sajedi\",\n          \"Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang\",\n          \"Seongsoo Heo, Dong-Wan Choi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Subjects\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"cs.CV, cs.AI\",\n          \"cs.CV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Subject_Tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"cs.CV, cs.AI\",\n          \"cs.CV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Colorectal cancer ranks among the most common and deadly cancers, emphasizing the need for effective early detection and treatment. To address the limitations of traditional colonoscopy, including high miss rates due to polyp variability, we introduce the Pyramid Vision Transformer Adapter Residual Network (PVTAdpNet). This model integrates a U-Net-style encoder-decoder structure with a Pyramid Vision Transformer backbone, novel residual blocks, and adapter-based skip connections. The design enhances feature extraction, dense prediction, and gradient flow, supported by squeeze-and-excitation attention for improved channel-wise feature refinement. PVTAdpNet achieves real-time, accurate polyp segmentation, demonstrating superior performance on benchmark datasets with high mDice and mIoU scores, making it highly suitable for clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851 and a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution polyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's capability for real-time, accurate performance within familiar distributions. The source code of our network is available at https://github.com/ayousefinejad/PVTAdpNet.git\",\n          \"Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\\\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Submitted\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Published: 2025-09-28T08:55:50Z | Updated: 2025-09-28T08:55:50Z\",\n          \"Published: 2025-09-25T10:29:47Z | Updated: 2025-09-29T00:07:53Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Detail Link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"http://arxiv.org/abs/2509.23751v1\",\n          \"http://arxiv.org/abs/2509.20986v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF Link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"http://arxiv.org/pdf/2509.23751v1\",\n          \"http://arxiv.org/pdf/2509.20986v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# Lưu file\n","save_results(api_results, \"arxiv_api.csv\", \"arxiv_api.jsonl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5vVhAJ8Hfxp","executionInfo":{"status":"ok","timestamp":1759227282648,"user_tz":-420,"elapsed":157,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"e7ab584e-dd55-4bb1-f988-2117bcfcdac7"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved 200 records -> arxiv_api.csv & arxiv_api.jsonl\n"]}]},{"cell_type":"code","source":["import json\n","from collections import OrderedDict\n","from datetime import datetime\n","\n","def to_display_date(iso_str: str) -> str:\n","    \"\"\"'2023-12-01T12:34:56Z' -> '1 December, 2023'\"\"\"\n","    try:\n","        dt = datetime.strptime(iso_str[:19], \"%Y-%m-%dT%H:%M:%S\")\n","        return dt.strftime(\"%-d %B, %Y\")\n","    except Exception:\n","        return iso_str\n","\n","def remap_item(item: dict) -> OrderedDict:\n","    \"\"\"\n","    ta tách ngày 'Published' để đổ vào 'Submitted Date'.\n","    \"\"\"\n","    submitted_date = item.get(\"Submitted\", \"\")\n","    if \"Published:\" in submitted_date:\n","        # tách 'Published: ISO | Updated: ISO'\n","        try:\n","            published_iso = submitted_date.split(\"Published:\")[1].split(\"|\")[0].strip()\n","            submitted_date = to_display_date(published_iso)\n","        except Exception:\n","            pass\n","\n","    return OrderedDict([\n","        (\"arXiv ID\",   item.get(\"arXiv ID\", \"\")),\n","        (\"PDF Link\",   item.get(\"PDF Link\", \"\")),\n","        (\"Subject_Tags\", item.get(\"Subject_Tags\", \"\")),\n","        (\"Subjects\",   item.get(\"Subjects\", \"\")),\n","        (\"Title\",      item.get(\"Title\", \"\")),\n","        (\"Authors\",    item.get(\"Authors\", \"\")),\n","        (\"Abstract\",   item.get(\"Abstract\", \"\")),\n","        (\"Submitted Date\", submitted_date),\n","    ])\n","\n","def save_pretty_json(items, path=\"arxiv_pretty.json\"):\n","    # Sắp xếp/đặt lại khóa theo mong muốn trước khi ghi\n","    remapped = [remap_item(x) for x in items]\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(remapped, f, ensure_ascii=False, indent=2)\n","    print(f\"Saved pretty JSON -> {path}\")\n","\n","# GỌI HÀM (api_results là list bạn đã có)\n","save_pretty_json(api_results, \"arxiv_pretty.json\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPS-IkHduscZ","executionInfo":{"status":"ok","timestamp":1759227315774,"user_tz":-420,"elapsed":27,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"b9eb2fc9-e167-45f2-f17f-da20e24503e2"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved pretty JSON -> arxiv_pretty.json\n"]}]},{"cell_type":"markdown","source":["# **TEXT PRE_PROCESSING**"],"metadata":{"id":"NXPmAF9qxoI_"}},{"cell_type":"code","source":["import nltk, pkgutil\n","\n","print(\"NLTK version:\", nltk.__version__)\n","\n","# Tải các gói cần thiết. 'punkt_tab' có ở NLTK >= 3.8, nên tải có kiểm tra.\n","for pkg in [\"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\"]:\n","    nltk.download(pkg, quiet=True)\n","\n","# Thử tải punkt_tab nếu có trong index\n","try:\n","    nltk.download(\"punkt_tab\", quiet=True)\n","except Exception as e:\n","    print(\"punkt_tab not available on this NLTK; continuing...\")\n","\n","print(\"NLTK data ready.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMnS_vkVxrq5","executionInfo":{"status":"ok","timestamp":1759227342943,"user_tz":-420,"elapsed":174,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"89b07467-1846-49ab-e16f-4461195087b0"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK version: 3.9.1\n","NLTK data ready.\n"]}]},{"cell_type":"code","source":["import re, string\n","from typing import List, Dict, Any\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","\n","# 1) Tokenize\n","def tokenize_text(text: str) -> List[str]:\n","    text = (text or \"\").lower()\n","    # bỏ URL/doi/links để sạch hơn (tuỳ chọn)\n","    text = re.sub(r'https?://\\S+|doi:\\S+', ' ', text)\n","    return word_tokenize(text)\n","\n","# 2) Clean: bỏ punctuation & stopwords & số (tuỳ chọn)\n","def clean_tokens(tokens: List[str]) -> List[str]:\n","    sw = set(stopwords.words('english'))\n","    punct = set(string.punctuation)\n","    cleaned = []\n","    for t in tokens:\n","        if t in punct:\n","            continue\n","        if t in sw:\n","            continue\n","        if t.isnumeric():  # bỏ token là số thuần\n","            continue\n","        cleaned.append(t)\n","    return cleaned\n","\n","# 3) Normalize: lemmatize hoặc stem\n","def normalize_tokens(tokens: List[str], option: str):\n","    if option == 'l':\n","        lem = WordNetLemmatizer()\n","        return [lem.lemmatize(t) for t in tokens]\n","    elif option == 's':\n","        stem = PorterStemmer()\n","        return [stem.stem(t) for t in tokens]\n","    else:\n","        raise ValueError(\"Invalid option. Use 'l' for lemmatization or 's' for stemming.\")\n","\n","# 4) Full pipeline cho một chuỗi\n","def preprocess_text(text: str, normalize: str = 'l') -> str:\n","    tokens = tokenize_text(text)\n","    cleaned = clean_tokens(tokens)\n","    normalized = normalize_tokens(cleaned, option=normalize)\n","    return ' '.join(normalized)\n","\n","# 5) Gộp metadata paper thành text rồi xử lý\n","def preprocess_paper(paper: Dict[str, Any], normalize: str = 'l') -> str:\n","    # Tương thích cả hai kiểu khóa: 'Submitted Date' (pretty JSON) hoặc 'Submitted' (API gốc)\n","    submitted = paper.get('Submitted Date') or paper.get('Submitted') or \"\"\n","    text_meta = \" \".join([\n","        paper.get('Title', ''),\n","        paper.get('Authors', ''),\n","        paper.get('Abstract', ''),\n","        paper.get('Subject_Tags', ''),\n","        paper.get('Subjects', ''),\n","        submitted\n","    ])\n","    return preprocess_text(text_meta, normalize=normalize)"],"metadata":{"id":"P01OowU-zm6B","executionInfo":{"status":"ok","timestamp":1759227346011,"user_tz":-420,"elapsed":4,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["#lấy bài theo keyword\n","keyword = \"transformer\"  # đổi theo ý bạn\n","paper = next((p for p in api_results if keyword.lower() in p.get(\"Title\",\"\").lower()), None)\n","if paper:\n","    print(\"arXiv ID:\", paper.get(\"arXiv ID\"))\n","    print(\"Title   :\", paper.get(\"Title\"))\n","else:\n","    print(\"Không tìm thấy bài nào chứa từ khóa:\", keyword)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKj_DKj7zPB1","executionInfo":{"status":"ok","timestamp":1759221980486,"user_tz":-420,"elapsed":24,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"2d8c595c-ffb3-442d-c0d8-1e399f34fd0e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["arXiv ID: 2509.24080v1\n","Title   : Ensembling Multilingual Transformers for Robust Sentiment Analysis of   Tweets\n"]}]},{"cell_type":"code","source":["import json, random, os\n","\n","# ---- 1) Load file JSON ----\n","json_path = \"arxiv_pretty.json\"  # đổi nếu bạn lưu tên khác\n","assert os.path.exists(json_path), f\"Không tìm thấy file: {json_path}\"\n","\n","with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","assert isinstance(data, list) and len(data) > 0, \"File JSON phải là một list các paper (ít nhất 1 phần tử).\"\n","\n","# ---- 2) Chọn 1 bài làm ví dụ ----\n","# Cách A: chọn theo index\n","idx = 0  # đổi số này để chọn bài khác, ví dụ 5, 10, ...\n","paper = data[idx]\n","\n","# (Tuỳ chọn) Cách B: chọn theo từ khóa trong Title\n","# keyword = \"transformer\"\n","# paper = next((p for p in data if keyword.lower() in p.get(\"Title\",\"\").lower()), data[0])\n","\n","print(\"arXiv ID:\", paper.get(\"arXiv ID\"))\n","print(\"Title   :\", paper.get(\"Title\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOjK10v4zZDi","executionInfo":{"status":"ok","timestamp":1759227354790,"user_tz":-420,"elapsed":22,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"a140a9c2-6c19-45d1-a94e-7234c4e878bd"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["arXiv ID: 2509.23859v1\n","Title   : FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for   Fair and Explainable Facial Beauty Prediction\n"]}]},{"cell_type":"code","source":["\n","# ---- 3) Hiển thị từng bước xử lý chỉ cho TITLE ----\n","title = paper.get(\"Title\", \"\")\n","\n","# B1: tokenize\n","tokens = tokenize_text(title)\n","print(\"\\n--- Tokens ---\")\n","print(tokens)\n","print(\"Total tokens:\", len(tokens))\n","\n","# B2: clean\n","cleaned = clean_tokens(tokens)\n","print(\"\\n--- Cleaned Tokens ---\")\n","print(cleaned)\n","print(\"Total cleaned:\", len(cleaned))\n","\n","# B3: normalize\n","lemmatized = normalize_tokens(cleaned, option='l')  # lemmatization\n","stemmed    = normalize_tokens(cleaned, option='s')  # stemming\n","print(\"\\n--- Lemmatized ---\")\n","print(lemmatized)\n","print(\"\\n--- Stemmed ---\")\n","print(stemmed)\n","\n","# B4: full preprocess cho TITLE\n","final_title = preprocess_text(title, normalize='l')  # đổi 's' nếu muốn stem\n","print(\"\\n=== Final Preprocessed Title (normalize='l') ===\")\n","print(final_title)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGbVYUgczgrF","executionInfo":{"status":"ok","timestamp":1759227359660,"user_tz":-420,"elapsed":34,"user":{"displayName":"Trương Lê Mỹ Thanh","userId":"02945880335582572386"}},"outputId":"6d0525e9-af35-413a-ca10-8d6e13de9a29"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Tokens ---\n","['fairvit-gan', ':', 'a', 'hybrid', 'vision', 'transformer', 'with', 'adversarial', 'debiasing', 'for', 'fair', 'and', 'explainable', 'facial', 'beauty', 'prediction']\n","Total tokens: 16\n","\n","--- Cleaned Tokens ---\n","['fairvit-gan', 'hybrid', 'vision', 'transformer', 'adversarial', 'debiasing', 'fair', 'explainable', 'facial', 'beauty', 'prediction']\n","Total cleaned: 11\n","\n","--- Lemmatized ---\n","['fairvit-gan', 'hybrid', 'vision', 'transformer', 'adversarial', 'debiasing', 'fair', 'explainable', 'facial', 'beauty', 'prediction']\n","\n","--- Stemmed ---\n","['fairvit-gan', 'hybrid', 'vision', 'transform', 'adversari', 'debias', 'fair', 'explain', 'facial', 'beauti', 'predict']\n","\n","=== Final Preprocessed Title (normalize='l') ===\n","fairvit-gan hybrid vision transformer adversarial debiasing fair explainable facial beauty prediction\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SbgS-81j16SF"},"execution_count":null,"outputs":[]}]}